{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification pt IIa - Logistic Regression and higher order decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_circles, make_moons, make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from math import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've been using logistic regression in terms of linear regression.  There is also a class in scikit-learn called `LogisticRegression`.  In order to fit it, it's using methods totally different than what we've used so far.  The method it uses involves defining a _cost function_, which you take the _derivative_ of and then perform some kind of _gradient-based optimization_.  Don't worry if that confuses you; we won't be dealing with any of that stuff in this term.\n",
    "\n",
    "There is one extra thing that's going on under the hood though, and that's __regularization__.  We'll discuss that on Friday, as well as the notion of _parameters_ and _hyperparameters_.\n",
    "\n",
    "You'll notice that this lab is `IIa`, because I need some time to consolidate all your collections of tweets into a single datafile.  We'll get to that as soon as I can!  For now, we have some practice that will warm us up to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Differences in linear and logistic regression scikit-learn models\n",
    "\n",
    "<div style=\"color:red\"> Look through the documentation, or use the method we did when inspecting the `LinearRegression` model in scikit-learn, to determine the similarities and differences in possible keyword arguments between scikit-learn objects `LinearRegression` and `LogisticRegression`.  Tell me which ones are shared, what they are, and which are different, and what _they_ are.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code and/or responses here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Exercises in decision boundaries\n",
    "\n",
    "<div style=\"color:red\"> For each of the following datasets, train a logistic regression classifier which can classify these points with reasonable accuracy.  </div>\n",
    "\n",
    "By reasonable here, that you should plot the decision boundary in scatterplot and make a convincing argument that you have accuractly fit the data.  Some thoughts on your process: you should probably\n",
    "\n",
    "1. Make a scatter plot, discuss what features you probably need and why,\n",
    "1. Perform whatever feature engineering you think you may need,\n",
    "1. Train a `LogisticRegression` classifier,\n",
    "1. Plot a new scatterplot including your decision boundary, and\n",
    "1. Repeat these steps until you have a good fit.\n",
    "\n",
    "The datasets are taken from `classification_exercises.csv` in the datasets folder on Canvas, and there are 5 datasets: `a`, `b`, `c`, `bonus`, and `otherbonus`.  The last two are bonuses, but they are not for extra points. They are simply challenge problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "You know what I hate about Fall?  I hate how early all the Christmas music comes out.  Why does it feel like it comes out earlier and earlier every year?  Let's make a classifier that can distinguish between lines of Christmas music lyrics, and the only thing I'm allowed to listen to these days: children's music lyrics.\n",
    "\n",
    "We'll try to perform this classification with a new type of classifier, Naive Bayes.  In the Naive Bayes classifier, the goal is the same, but the process is completely different.  Why might you care about this?  The reason is the __assumptions__ of the model.  For example, the assumptions of linear regression are:\n",
    "\n",
    "* Linear relationship: Hopefully this one is obvious.  The predictors should be linearly related to the response (perhaps after feature engineering)\n",
    "* Multivariate normality: the predictors are taken from a multivariate normal distribution (often not true, but not a huge issue)\n",
    "* No or little multicollinearity: essentially, a generalization of what it would mean for a non-square matrix to have a non-zero determinant.  This is not a good explanation, and it isn't quite true, but this is also a bit of a technicality that we shouldn't concern ourselves with.\n",
    "* No auto-correlation in the errors: We've discussed this one!\n",
    "* Homoscedasticity: If you happen to have different response variables (we never have), then they have the same variance.\n",
    "\n",
    "<div style=\"color:red\"> To start, do some searching around online to see if you can determine what the assumptions of Logistic Regression and Naive Bayes are. Why might Naive Bayes be a good choice for analyzing lines of song lyrics?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Naive Bayes?\n",
    "\n",
    "Before we answer that, we need to establish some terminology: in this setting, a __document__ is a single line of lyrics, and the __corpus__ is the total collection of all lyrics.\n",
    "\n",
    "Based on the assumptions you determined above, there is a somewhat-standard-but-too-much-probability-for-me-today derivation of the Naive Bayes algorithm.  What you arrive at is the very believable equation:\n",
    "\n",
    "$$p\\left(C_k \\mid x_1, \\ldots, x_n\\right) = \\frac{1}{Z}\\ p\\left(C_k\\right)\\prod_{i=1}^n p\\left(x_i \\mid C_k\\right)$$\n",
    "\n",
    "(\"Doesn't look very believable to me,\" you could rightly say.) This is far easier to understand than it looks.  What are all the pieces of this equation?\n",
    "\n",
    "* $p\\left(C_k \\mid x_1, \\ldots, x_n\\right)$ is the __conditional probability__ of the vector $\\overline{v} = \\left<x_1, \\ldots, x_n\\right>$ being in class $C_k$.  More simply, it's the chance that the document you're currently looking at is in the class you're currently considering it to be in.  You compute this probability for all the classes, and then just choose the largest-probability class for $\\overline{v}$.\n",
    "* $\\frac{1}{Z}$ is a scaling factor used to make everything add up to 1, because probabilities always have to add to one.  Since it doesn't depend on the class, it's of no interest to us. (If you had to pick the bigger of two things: $y = 2x$ and $z = 3x$, for some positive number x, you don't care what $x$ is, you know $z$ is automatically the winner. So you can just ignore the $x$.)\n",
    "* $p\\left(C_k\\right)$ is the __prior probability__ of class $C_k$.  This is a way fancier name than it needs to be: it's the fraction of total documents that are in class $C_k$.\n",
    "* $\\prod_{i=1}^n$ is the __product__ symbol.  Just like $\\sum_{i=1}^n$ is the sum symbol.  It just means \"multiply up all the following:\".\n",
    "* $p\\left(x_i \\mid C_k\\right)$ is the __likelihood__ of $x_i$ being in class $C_k$.  Again, it's just a fraction: it's the fraction of those documents in class $C_k$ which actually contain $x_i$.  If there are 5 tweets that are classified as \"hangry\", and 2 of them contain the word \"eat\", then the likelihood $p\\left(\\text{\"eat\"} \\mid \\text{\"hangry\"}\\right) = \\frac{2}{5}$. It doesn't matter for this calculation that there were also 3 occurances of \"eat\" that were classified as \"not hangry\", or that the \"not hangry\" class had 25 tweets it in.\n",
    "\n",
    "So, putting this all together, you need to calculate all the $p\\left(C_k\\right)$ terms for each class, and the $\\prod_{i=1}^n p\\left(x_i \\mid C_k\\right)$ terms for each (document, class) pair, and then given any document, you classify a document (for training or for new, test data) by computing all the conditional probabilities for each class and picking the class with the highest probability.  \n",
    "\n",
    "#### Okay, so: your turn.  \n",
    "<div style=\"color:red\"> Make a Naive Bayes classifier on your dataset of \"lines from songs\", classifying them as children's music or Christmas music.</div>\n",
    "\n",
    "The datasets should be taken from `christmas_music.txt` and `non_christmas_music.txt` on Canvas.\n",
    "\n",
    "Here's a potentially incomplete list of tasks to do:\n",
    "* read in the text files,\n",
    "* clean them up appropriately (for example in song lyrics, some words have shortenings like `e'erywhere` or `singin'` that you should probably expand before stemming),\n",
    "* vectorize/stem the words, remove stopwords,\n",
    "* (you may need to build a special stopword list over time: filter out your current list of stop words, view the words that are in a ton of documents, add those your list of stop words, filter again, etc.)\n",
    "* create a bunch of counts of the \"total collection of documents\", \"total collection of documents in class 1 (Christmas)\", \"total collection of documents in class 0 (Children's)\",\n",
    "* create two global variables: the first is `CORPUS_SIZE`, its the number of words to include from your _corpus_, or body of documents.  The second is below: `BIGRAM_MULTIPLIER`.\n",
    "* <span style=\"color:red\"> Once you successfully do create the model, do the same thing again inluding the top `BIGRAM_MULTIPLIER * CORPUS_SIZE` bigrams in your corpus.  Bigrams are pairs of words in succession.</span>\n",
    "\n",
    "#### Note: this may look disturbingly similar to what we're going to do on the tweet dataset!  That's the point!  This is practice for that, and a lot of the code you write should carry over just fine.\n",
    "\n",
    "#### Note 2: A lot of the above can be done using `nltk`.  Use it! This lab would be way too long without using that library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
